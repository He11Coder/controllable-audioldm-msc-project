import os
import torch
import torchaudio
import json
import argparse
import numpy as np
from frechet_audio_distance import FrechetAudioDistance
from scipy.stats import entropy


class ObjEvaluator:
    """
    A class to handle the comprehensive objective evaluation of samples generated by a conditional vocoder.
    """
    def __init__(self, native_sampling_rate):
        """
        Initializes the evaluator.
        """
        self.native_sr = native_sampling_rate
        
        # Load the model used for FAD and other metrics
        self.fad_model = FrechetAudioDistance(model_name="vggish", use_pca=False, use_activation=False, verbose=False)

    def generate_samples_for_eval(self, raw_fake_dir, real_dir, output_dir):
        os.makedirs(output_dir, exist_ok=False)

        for filename in os.listdir(real_dir):
            real_path = os.path.join(real_dir, filename)
            try:
                real_waveform, sr = torchaudio.load(real_path)
            except Exception as e:
                print(f'Failed to read {filename}: {e}')

            gen_path = os.path.join(raw_fake_dir, f"{filename}_{sr}")
            try:
                gen_waveform, _ = torchaudio.load(gen_path)
            except Exception as e:
                print(f'Failed to read {filename}_{sr}: {e}')

            # Downsample the generated waveform to the target rate
            resampler_gen = torchaudio.transforms.Resample(self.native_sr, sr).to(self.device)
            generated_waveform_target = resampler_gen(gen_waveform)

            torchaudio.save(os.path.join(output_dir, f"{filename}.wav"), generated_waveform_target, sr)

    def calculate_metrics(self, real_dir, fake_dir):
        """
        Calculates FAD, IS, and KL Divergence.
        """
        print("Calculating metrics...")
        # Calculate FAD score
        fad_score = self.fad_model.score(real_dir, fake_dir)

        # Get embeddings for IS and KL
        real_embeddings = self.fad_model.get_embeddings(real_dir)
        fake_embeddings = self.fad_model.get_embeddings(fake_dir)

        # Calculate KL Divergence
        # Create histograms (discrete probability distributions) from the embeddings
        real_hist, _ = np.histogram(real_embeddings.mean(axis=1), bins=5, density=True)
        fake_hist, _ = np.histogram(fake_embeddings.mean(axis=1), bins=5, density=True)

        # Add a small epsilon to avoid division by zero
        real_hist += 1e-10
        fake_hist += 1e-10
        kl_score = entropy(pk=fake_hist, qk=real_hist)

        # Calculate Inception Score (IS)
        # This is a simplified version for audio. It measures the KL divergence
        # of the marginal distribution from a uniform distribution.
        # A higher score indicates more confident and diverse predictions.
        preds = torch.nn.functional.softmax(torch.from_numpy(fake_embeddings), dim=1)
        marginal_dist = preds.mean(dim=0)
        is_scores = []
        for i in range(preds.size(0)):
            is_scores.append(entropy(preds[i], marginal_dist))
        is_score = np.exp(np.mean(is_scores))

        return {"fad": fad_score, "kl_divergence": kl_score, "inception_score": is_score}
    
    def run_full_evaluation(self, fake_dir, real_dir, run_name):
        """
        Orchestrates the entire evaluation process for all supported sample rates.
        """
        all_results = {}
        output_dir = f'./fake_{run_name}'
        self.generate_samples_for_eval(fake_dir, real_dir, output_dir)
        metrics = self.calculate_metrics(real_dir, output_dir)
        all_results[0] = metrics
        
        # Save final results to a file
        results_path = f"final_evaluation_results_{run_name}.json"
        with open(results_path, "w") as f:
            json.dump(all_results, f, indent=4)
        print(f"\nFull evaluation complete. Results saved to {results_path}")


parser = argparse.ArgumentParser()

parser.add_argument('--run_name', type=str, required=True)
parser.add_argument('--real_dir', type=str, required=True)
parser.add_argument('--fake_dir', type=str, required=True)
parser.add_argument('--native_sr', type=int, required=True)
args = parser.parse_args()

evaluator = ObjEvaluator(native_sampling_rate=args.native_sr)
evaluator.run_full_evaluation(args.fake_dir, args.real_dir, args.run_name)